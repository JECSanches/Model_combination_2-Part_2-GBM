{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c050e0a-c15e-4fc8-bd4a-16fcd8951c69",
   "metadata": {},
   "source": [
    "# Combinação de modelos - Parte 2 (GBM - Gradient boosting machines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f35a843-a011-4e7b-b13e-e21966b1e975",
   "metadata": {},
   "source": [
    "1. Cite 5 diferenças entre o AdaBoost e o GBM (Gradient boosting machines).\n",
    "\n",
    "- O GBM inicia com a média da variável resposta/target, diferentemente do AdaBoost que inicia com a geração de Stumps;\n",
    "  \n",
    "- A técnica GBM utiliza florestas de árvore que podem possuir variados valores de profundidades e número mínimo de amostras por folha. O AdaBoost utiliza uma floresta de stumps, ou seja, árvores de profundidade 1 e número mínimo de amostras por folha igual a 2;\n",
    "\n",
    "- Todas as respostas possuem um multiplicador em comum chamado *learning-rate* (*eta*) e não pesos diferentes para cada observação (como é feito do Adaboost);\n",
    "\n",
    "- O modelo GBM faz uso de resíduos. Diferentemente do AdaBoost, que ajusta os pesos das instâncias a cada observação, o GBM busca ajustar o novo preditor aos erros residuais cometidos pelo preditor anterior;\n",
    "\n",
    "- Um outro ponto que destaca a diferença entre as duas técnicas é a forma como ambas abordam as saídas dos modelos. Para o AdaBoost cada resposta tem um peso diferente, para o GBM as respostas tem um multiplicador comum e o adicional de que cada resposta subsequente é ajustada com base na resposta anterior para o cálculo do resíduo. Vamos considerar o cálculo do quarto resíduo na sequência do modelo (caso hipotético), assim teremos: $resíduo_4 = y - [predito_1 + eta*predito_2 + eta*predito_3]$, sendo que o valor predito_1 é a média da variável resposta (isso para análises de regressão)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8560af47-3e1b-4470-a141-ed39a93036f5",
   "metadata": {},
   "source": [
    "2. Acesse o [Scikit-learn – GBM](https://scikit-learn.org/stable/modules/ensemble.html), leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c15452f-c9bb-4866-8c16-ef3492a5647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eff3cfe-4bdd-46b0-8743-0ca9a073c5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 297 ms\n",
      "Wall time: 301 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Testando o GBM classifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b510f7-d759-45f3-bc6f-a0875bce85a1",
   "metadata": {},
   "source": [
    "3. (Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a48a3cd-243e-4a0e-9d00-74a84acce077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'learning_rate': 0.06, 'min_samples_leaf': 16, 'n_estimators': 901}\n",
      "CPU times: total: 17min 10s\n",
      "Wall time: 17min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9155"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "parametros = {\n",
    "    'n_estimators': list(range(1, 1001, 100)),\n",
    "    'min_samples_leaf': list(range(1, 50, 5)),\n",
    "    'learning_rate': [0.04, 0.06, .1]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=gb,\n",
    "                    param_grid=parametros,\n",
    "                    scoring='roc_auc',\n",
    "                    verbose=False,\n",
    "                    cv=2)\n",
    "\n",
    "grid.fit(X_train, y_train.ravel())\n",
    "\n",
    "best_params = grid.best_params_\n",
    "print(f'Melhores parâmetros: {best_params}')\n",
    "\n",
    "\n",
    "score = grid.best_estimator_.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829ad76f-717a-47f4-a3b6-5917b8f7c4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5eeff42b-2d2a-4480-851f-274c8881eef5",
   "metadata": {},
   "source": [
    "3. Cite 5 hiperparâmetros importantes no GBM\n",
    "\n",
    "Os hiperparâmetros do método são semelhantes aos do AdaBoost.\n",
    "- **loss**: *{‘squared_error’, ‘absolute_error’, ‘huber’, ‘quantile’}, default=’squared_error’* - É a função de perda a ser otimizada;\n",
    "- **learning_rate**: Reduz a contribuição de cada árvore;\n",
    "- **n_estimator**: Representa o número de estágios de boosting a serem realizados. O GBM é robusto ao overfitting e um maior o número de estimadores/árvores geralmente resulta em melhor desempenho;\n",
    "- **subsample**: A fração das amostras a serem utilizadas para ajustar os *weak learners* ou *base learners*;\n",
    "- **criterion**: *{‘friedman_mse’, ‘squared_error’}, default=’friedman_mse’* - Função utilizada para medir a qualidade de uma divisão, um *split*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2eb10d-8eb4-4561-8fb8-6fca6b052b30",
   "metadata": {},
   "source": [
    "5. Acessando o artigo do Jerome Friedman ([Stochastic](https://jerryfriedman.su.domains/ftp/stobst.pdf)) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?\n",
    "\n",
    "- Basicamente a diferença entre os dois algoritmos se encontra no treinamento dos *weak lerners*. O GBM utiliza todos os dados de treino nessa tarefa, já o *stochastic gradient boosting* (SGBM) utiliza uma subamostragem dos dados de treino para o desenvolvimento dos *weak lerners*. Esse último é capaz de aprimorar o retorno em função da adição de aleatoriedade e variância, pois as subamostras tendem a ser menos correlacionadas, o que também ajuda a evitar o overfitting no algoritmo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b732cb-9c30-4e79-95a4-19feb873f702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
